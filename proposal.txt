The collection of data is widespread across all organizations, but presents a unique challenge for small non-profit organizations. Although demonstrating impact is a priority, often the resources, time, and expertise needed to collect and analyze data are often not present. That said, the stakes for not doing so are high in an industry moving toward a data-informed model. 

WorldTeach, a small non-profit based in Cambridge, MA, faces these same challenges. Data collection practices have improved, but still are not perfect, and analyzing and using the data is very limited, typically consisting of a Program Director reviewing individual country and cohort data year to year with in-country Field Directors. No attempt has been made to analyze across programs or over time or as a whole. 

WorldTeach surveys volunteers at 3 points during their year of volunteer service: Following Orientation, MId-Service, and End-of-Service conferences. WT redid their surveys in 2016 to improve data collection and what is collected. Past data is available, but not all questions will be the same. In the survey data since redoing the surveys in 2016, there are currently 158 Orientation responses, 138 Mid-Service, and 103 End-of-Service responses. This is over 2 years of programs in 6 countries. There is the same data for 8 week Summer Programs over 6 countries, but with much less responses. The completion rate is around 45% for Orientation and Mid-Service, but significantly lower for End of Service, and not all cohorts in each survey have completed all surveys if their program is ongoing. 

We would like to look at the survey data we have from volunteers across the 3 surveys to see if we can see any changes or growth in respect to WT's mission (global citizenship, professional development, cross-cultural understanding, etc.), as well as look for trends across programs in regards to other info that is there like health & safety, teaching conditions, etc. There is a huge variety of questions within each survey, and again WT has really never tried to look to look at some of the potential growth metrics across surveys or compared the data across cohorts and programs. In addition to analyzing the data, it would also be interesting to look at better metrics and methods of of collecting data in the future to better attempt to measure impact. Identifying what is missing or could be improved would be valuable. 

With less than 4 weeks to the deadline, there is a lot of work to do. First is analyzing the data and really understanding what is there and what is interesting to look at. Ideally, finding ways to look at changes from Orientation to End of Service would be great, but having not compared all 3 surveys, that is to be determined, although there is one question that asks self reported data on resilience, cross-cultural, etc across all surveys. The bulk of the work is planning the visualizations and what story we want to try and extract. Because the data covers so many topics and changes from survey to survey, there are a lot of possibilities. After determining visualizations, we will attempt to prototype and visualize the data using Tableau, and other programming tools. This final step is a little concerning as both both Bryan and I have not been the experts in coding while working in other groups, but we will give it our best shot. 
